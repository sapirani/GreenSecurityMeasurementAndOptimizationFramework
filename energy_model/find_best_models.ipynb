{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "from energy_model.configs.columns import SystemColumns, ProcessColumns\n",
    "from energy_model.pipelines.pipeline_utils import extract_x_y\n",
    "import pandas as pd\n",
    "from typing import Any\n",
    "import numpy as np\n",
    "\n",
    "from energy_model.energy_model_parameters import PROCESS_SYSTEM_DF_PATH, SYSTEM_ONLY_DF_PATH\n",
    "from energy_model.pipelines.grid_search_pipeline_executor import GridSearchPipelineExecutor\n",
    "\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.linear_model import BayesianRidge\n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.linear_model import ARDRegression\n",
    "from sklearn.linear_model import HuberRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import PassiveAggressiveRegressor\n",
    "from sklearn.linear_model import TheilSenRegressor\n",
    "from sklearn.linear_model import RANSACRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.tree import ExtraTreeRegressor\n",
    "from sklearn.dummy import DummyRegressor\n",
    "from sklearn.linear_model import ElasticNetCV\n",
    "from sklearn.linear_model import Lars\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "from sklearn.linear_model import LarsCV\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.linear_model import LassoLars\n",
    "from sklearn.linear_model import LassoLarsCV\n",
    "from sklearn.linear_model import LassoLarsIC\n",
    "from sklearn.svm import LinearSVR\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.compose import TransformedTargetRegressor\n",
    "from sklearn.linear_model import TweedieRegressor\n",
    "from sklearn.linear_model import SGDRegressor\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Models",
   "id": "50a5480bf362cfb1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "GradientBoostingRegressorModel = {\"classifier\": [GradientBoostingRegressor()],\n",
    "                                  \"classifier__loss\": [\"squared_error\", \"huber\"],\n",
    "                                  'classifier__max_depth': [80, 110],\n",
    "                                  'classifier__max_features': [3],\n",
    "                                  'classifier__min_samples_leaf': [3, 5],\n",
    "                                  'classifier__min_samples_split': [8, 12],\n",
    "                                  'classifier__n_estimators': [100, 500, 1000]}\n",
    "\n",
    "ExtraTreeRegressorModel = {\"classifier\": [ExtraTreeRegressor()],\n",
    "                           # 'classifier__n_estimators': [10, 50, 100],\n",
    "                           'classifier__criterion': ['squared_error', 'absolute_error'],\n",
    "                           'classifier__max_depth': [2, 16,50],\n",
    "                           'classifier__min_samples_split': [2, 6],\n",
    "                           'classifier__min_samples_leaf': [1, 2],\n",
    "                           # 'oob_score': [True, False],\n",
    "                           'classifier__max_features': ['sqrt']}\n",
    "# ElasticNetModel = {\"classifier\": [ElasticNet()],\n",
    "#                    \"classifier__max_iter\": [5, 50],\n",
    "#                    \"classifier__alpha\": [0.001, 0.01, 0.1],\n",
    "#                    \"classifier__l1_ratio\": np.arange(0.0, 1.0, 0.1)}\n",
    "\n",
    "HistGradientBoostingRegressorModel = {\"classifier\": [HistGradientBoostingRegressor()],\n",
    "                                        \"classifier__loss\": [\"squared_error\", \"quantile\"],\n",
    "                                      \"classifier__quantile\": [0.5, 0.6, 0.7],\n",
    "                                      \"classifier__max_iter\": [400, 600, 800],\n",
    "                                      \"classifier__l2_regularization\": [0.1, 0.3, 1.0, 3.0],\n",
    "                                      'classifier__max_depth': [3, 4, 5, 6, 8],#range(5, 16, 2),\n",
    "                                      'classifier__min_samples_leaf': [20, 50, 100, 200]} #range(10, 100, 10)}\n",
    "ExtraTreesRegressorModel = {\"classifier\": [ExtraTreesRegressor()],\n",
    "                            \"classifier__max_depth\": [3, 5, 7, 12],\n",
    "                            \"classifier__min_samples_leaf\": [3, 7],\n",
    "                            \"classifier__min_weight_fraction_leaf\": [0.1, 0.5],\n",
    "                            \"classifier__max_features\": [\"sqrt\"],\n",
    "                            \"classifier__max_leaf_nodes\": [10, 60, 90]}\n",
    "\n",
    "RandomForestRegressorModel = {\"classifier\": [RandomForestRegressor()],\n",
    "                              'classifier__n_estimators': [50, 100, 500, 1000],\n",
    "                              'classifier__max_features': ['sqrt'],\n",
    "                              'classifier__max_depth': [5, 7, 15, 60],\n",
    "                              'classifier__min_samples_split': [2, 5, 10],\n",
    "                              'classifier__min_samples_leaf': [1, 4]}"
   ],
   "id": "f597b0b9afa1f1be",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "all_possible_models = {\n",
    "    \"GradientBoostingRegressorModel\": GradientBoostingRegressorModel,\n",
    "    \"ExtraTreesRegressorModel\": ExtraTreesRegressorModel,\n",
    "    \"ExtraTreeRegressorModel\": ExtraTreeRegressorModel,\n",
    "    \"HistGradientBoostingRegressorModel\": HistGradientBoostingRegressorModel,\n",
    "    \"RandomForestRegressorModel\": RandomForestRegressorModel\n",
    "}\n",
    "\n",
    "system_fine_tune_model = {\n",
    "    \"HistGradientBoostingRegressorModel\": HistGradientBoostingRegressorModel}"
   ],
   "id": "f20bd833c9a8c0c3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "process_df_path = PROCESS_SYSTEM_DF_PATH\n",
    "system_only_df_path = SYSTEM_ONLY_DF_PATH\n",
    "\n",
    "system_target = SystemColumns.ENERGY_USAGE_SYSTEM_COL\n",
    "process_target = ProcessColumns.ENERGY_USAGE_PROCESS_COL"
   ],
   "id": "3aa60c3dcac57a20",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from typing import Callable\n",
    "\n",
    "\n",
    "def run_grid_search(target_col: str, dataset_path: str, possible_models: list[dict], scoring_methods: dict[str, str | Callable]) -> dict[str, Any]:\n",
    "    grid_search_pipeline = GridSearchPipelineExecutor(possible_models=possible_models, scoring_methods=scoring_methods)\n",
    "    dataset = pd.read_csv(dataset_path, index_col=0)\n",
    "    X, y = extract_x_y(dataset, target_column=target_col)\n",
    "    best_model_per_metric = grid_search_pipeline.run_grid_search(X, y)\n",
    "    return best_model_per_metric"
   ],
   "id": "153d7fff2f3f6c6d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def print_best_models(best_model: dict[str, Any], model_name: str) -> str:\n",
    "    res = f\"\\n\\nGrid Search Results for Model {model_name}: \\n{best_model}\"\n",
    "    print(res)\n",
    "    return res"
   ],
   "id": "18b813ac3d7a04c8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def run_grid_search_on_all_models(target_col: str, dataset_path: str, model_options: dict[str, dict[str, Any]], scoring_methods: dict[str, str | Callable]) -> tuple[dict[str, dict[str, Any]], str]:\n",
    "    best_model_per_type = {}\n",
    "    full_results = \"\"\n",
    "    for model_name, model in model_options.items():\n",
    "        full_results += f\"\\n\\n***** Starting Grid Search for Model {model_name}: *****\\n\"\n",
    "        print(f\"\\n\\n***** Starting Grid Search for Model {model_name}: *****\\n\")\n",
    "        best_model_per_metric = run_grid_search(target_col, dataset_path, [model], scoring_methods)\n",
    "        res = print_best_models(best_model_per_metric, model_name)\n",
    "        full_results += res\n",
    "        best_model_per_type[model_name] = best_model_per_metric\n",
    "        print(f\"\\n\\n***** Finished Grid Search for Model {model_name}: *****\\n\")\n",
    "        full_results += f\"\\n\\n***** Finished Grid Search for Model {model_name}: *****\\n\"\n",
    "\n",
    "    return best_model_per_type, full_results\n"
   ],
   "id": "f1945f05acbf4da9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Additional methods for metrics and loss functions",
   "id": "8813f72cecddd6b5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "def negative_penalty(y_pred):\n",
    "    y_pred = np.asarray(y_pred)\n",
    "    neg = y_pred[y_pred < 0]\n",
    "    if len(neg) == 0:\n",
    "        return 0.0\n",
    "    return np.mean(np.abs(neg))\n",
    "\n",
    "\n",
    "def tail_rmse(y_true, y_pred, percentile=95):\n",
    "    y_true = np.asarray(y_true)\n",
    "    y_pred = np.asarray(y_pred)\n",
    "\n",
    "    squared_errors = (y_true - y_pred) ** 2\n",
    "    threshold = np.percentile(squared_errors, percentile)\n",
    "    tail_errors = squared_errors[squared_errors >= threshold]\n",
    "\n",
    "    return np.sqrt(np.mean(tail_errors))\n",
    "\n",
    "def combined_tail_rmse_and_negative_penalty_loss(y_true, y_pred, percentile=95, lambda_neg=10.0) -> float:\n",
    "    tail = tail_rmse(y_true, y_pred, percentile)\n",
    "    neg_pen = negative_penalty(y_pred)\n",
    "    return tail + lambda_neg * neg_pen\n",
    "\n",
    "\n",
    "def system_additional_scorer(percentile=95, lambda_neg=5.0):\n",
    "    return make_scorer(\n",
    "        lambda y_true, y_pred: -combined_tail_rmse_and_negative_penalty_loss(\n",
    "            y_true, y_pred,\n",
    "            percentile=percentile,\n",
    "            lambda_neg=lambda_neg\n",
    "        ),\n",
    "        greater_is_better=True\n",
    "    )\n"
   ],
   "id": "de5ab3b4bcb0861c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def smape(y_true, y_pred, eps=1e-8):\n",
    "    y_true = np.asarray(y_true)\n",
    "    y_pred = np.asarray(y_pred)\n",
    "\n",
    "    numerator = np.abs(y_pred - y_true)\n",
    "    denominator = (np.abs(y_true) + np.abs(y_pred)) / 2.0 + eps\n",
    "\n",
    "    return np.mean(numerator / denominator) * 100\n",
    "\n",
    "def combined_smape_and_negative_penalty_loss(y_true, y_pred, lambda_neg=5.0):\n",
    "    s = smape(y_true, y_pred)\n",
    "    neg_pen = negative_penalty(y_pred)\n",
    "    return s + lambda_neg * neg_pen\n",
    "\n",
    "\n",
    "def process_additional_scorer(lambda_neg=5.0):\n",
    "    return make_scorer(\n",
    "        lambda y_true, y_pred: -combined_smape_and_negative_penalty_loss(\n",
    "            y_true, y_pred,\n",
    "            lambda_neg=lambda_neg\n",
    "        ),\n",
    "        greater_is_better=True\n",
    "    )"
   ],
   "id": "5a6d8804c31e56cc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Find best system energy model",
   "id": "5002b3bcd9efd49f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "system_additional_scoring = system_additional_scorer()\n",
    "system_scoring_methods = {\n",
    "    \"neg_mean_squared_error\": \"neg_mean_squared_error\",\n",
    "    \"neg_root_mean_squared_error\": \"neg_root_mean_squared_error\",\n",
    "    \"tail_rmse_and_negative_penalty\": system_additional_scoring\n",
    "}"
   ],
   "id": "d5a2c19ff24435a7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "best_system_models, results_system_txt = run_grid_search_on_all_models(system_target, system_only_df_path, all_possible_models, system_scoring_methods)",
   "id": "a76a7f487c61ca13",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Fine tune system model",
   "id": "fb03d1aca81ed886"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "system_additional_scoring = system_additional_scorer()\n",
    "system_updated_scoring_methods = {\n",
    "    \"neg_mean_squared_error\": \"neg_mean_squared_error\",\n",
    "    \"neg_root_mean_squared_error\": \"neg_root_mean_squared_error\",\n",
    "    \"tail_rmse_and_negative_penalty\": system_additional_scoring,\n",
    "    \"tail_rmse\": make_scorer(tail_rmse, greater_is_better=False),\n",
    "}"
   ],
   "id": "b8c0fbcaa1f02b00",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "best_system_models_finetune, results_system_txt_finetune = run_grid_search_on_all_models(system_target, system_only_df_path, system_fine_tune_model, system_updated_scoring_methods)",
   "id": "b558133e92d46cd2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "with open(f\"finetune_system_results.txt\", \"w\") as f:\n",
    "    f.write(results_system_txt)"
   ],
   "id": "1f1855121e9c58f8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Find Best Process Energy Model",
   "id": "14a3f6bdddaa61f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "process_additional_scoring = process_additional_scorer()\n",
    "process_scoring_methods = {\n",
    "    \"neg_mean_squared_error\": \"neg_mean_squared_error\",\n",
    "    \"neg_root_mean_squared_error\": \"neg_root_mean_squared_error\",\n",
    "    \"tail_rmse_and_negative_penalty\": process_additional_scoring\n",
    "}"
   ],
   "id": "ef40d64d6444c6e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "best_process_models, results_process_txt = run_grid_search_on_all_models(process_target, process_df_path, all_possible_models, process_scoring_methods)",
   "id": "446e1c859d2b40cd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "with open(f\"full_results_process.txt\", \"w\") as f:\n",
    "    f.write(results_process_txt)"
   ],
   "id": "fc5dbb0f0503c07c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "5b3f7eddad3064de",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
